{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edca37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIME Analysis - Insitu Surface Porosity Predictions\n",
    "\n",
    "import types\n",
    "from lime.utils.generic_utils import has_arg\n",
    "from skimage.segmentation import felzenszwalb, slic, quickshift\n",
    "import copy\n",
    "from functools import partial\n",
    "\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "from sklearn.utils import check_random_state\n",
    "from skimage.color import gray2rgb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import scipy.ndimage as ndi\n",
    "from skimage.segmentation._quickshift_cy import _quickshift_cython\n",
    "\n",
    "from lime import lime_base\n",
    "from lime.wrappers.scikit_image import SegmentationAlgorithm\n",
    "\n",
    "import skimage\n",
    "from matplotlib import colors\n",
    "from skimage.segmentation import mark_boundaries, find_boundaries\n",
    "from skimage.morphology import dilation,square\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class BaseWrapper(object):\n",
    "    \"\"\"Base class for LIME Scikit-Image wrapper\n",
    "    Args:\n",
    "        target_fn: callable function or class instance\n",
    "        target_params: dict, parameters to pass to the target_fn\n",
    "    'target_params' takes parameters required to instanciate the\n",
    "        desired Scikit-Image class/model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target_fn=None, **target_params):\n",
    "        self.target_fn = target_fn\n",
    "        self.target_params = target_params\n",
    "\n",
    "        self.target_fn = target_fn\n",
    "        self.target_params = target_params\n",
    "\n",
    "    def _check_params(self, parameters):\n",
    "        \"\"\"Checks for mistakes in 'parameters'\n",
    "        Args :\n",
    "            parameters: dict, parameters to be checked\n",
    "        Raises :\n",
    "            ValueError: if any parameter is not a valid argument for the target function\n",
    "                or the target function is not defined\n",
    "            TypeError: if argument parameters is not iterable\n",
    "         \"\"\"\n",
    "        a_valid_fn = []\n",
    "        if self.target_fn is None:\n",
    "            if callable(self):\n",
    "                a_valid_fn.append(self.__call__)\n",
    "            else:\n",
    "                raise TypeError('invalid argument: tested object is not callable,\\\n",
    "                 please provide a valid target_fn')\n",
    "        elif isinstance(self.target_fn, types.FunctionType) \\\n",
    "                or isinstance(self.target_fn, types.MethodType):\n",
    "            a_valid_fn.append(self.target_fn)\n",
    "        else:\n",
    "            a_valid_fn.append(self.target_fn.__call__)\n",
    "\n",
    "        if not isinstance(parameters, str):\n",
    "            for p in parameters:\n",
    "                for fn in a_valid_fn:\n",
    "                    if has_arg(fn, p):\n",
    "                        pass\n",
    "                    else:\n",
    "                        raise ValueError('{} is not a valid parameter'.format(p))\n",
    "        else:\n",
    "            raise TypeError('invalid argument: list or dictionnary expected')\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        \"\"\"Sets the parameters of this estimator.\n",
    "        Args:\n",
    "            **params: Dictionary of parameter names mapped to their values.\n",
    "        Raises :\n",
    "            ValueError: if any parameter is not a valid argument\n",
    "                for the target function\n",
    "        \"\"\"\n",
    "        self._check_params(params)\n",
    "        self.target_params = params\n",
    "\n",
    "    def filter_params(self, fn, override=None):\n",
    "        \"\"\"Filters `target_params` and return those in `fn`'s arguments.\n",
    "        Args:\n",
    "            fn : arbitrary function\n",
    "            override: dict, values to override target_params\n",
    "        Returns:\n",
    "            result : dict, dictionary containing variables\n",
    "            in both target_params and fn's arguments.\n",
    "        \"\"\"\n",
    "        override = override or {}\n",
    "        result = {}\n",
    "        for name, value in self.target_params.items():\n",
    "            if has_arg(fn, name):\n",
    "                result.update({name: value})\n",
    "        result.update(override)\n",
    "        return result\n",
    "\n",
    "\n",
    "class SegmentationAlgorithm(BaseWrapper):\n",
    "      \"\"\" Define the image segmentation function based on Scikit-Image\n",
    "           implementation and a set of provided parameters\n",
    "          Args:\n",
    "             algo_type: string, segmentation algorithm among the following:\n",
    "                 'quickshift', 'slic', 'felzenszwalb'\n",
    "             target_params: dict, algorithm parameters (valid model paramters\n",
    "                 as define in Scikit-Image documentation)\n",
    "      \"\"\"\n",
    "\n",
    "def __init__(self, algo_type, **target_params):\n",
    "       \n",
    "    self.algo_type = algo_type\n",
    "    if (self.algo_type == 'quickshift'):\n",
    "            BaseWrapper.__init__(self, quickshift, **target_params)\n",
    "            kwargs = self.filter_params(quickshift)\n",
    "            self.set_params(**kwargs)\n",
    "    elif (self.algo_type == 'felzenszwalb'):\n",
    "            BaseWrapper.__init__(self, felzenszwalb, **target_params)\n",
    "            kwargs = self.filter_params(felzenszwalb)\n",
    "            self.set_params(**kwargs)\n",
    "    elif (self.algo_type == 'slic'):\n",
    "            BaseWrapper.__init__(self, slic, **target_params)\n",
    "            kwargs = self.filter_params(slic)\n",
    "            self.set_params(**kwargs)\n",
    "\n",
    "def __call__(self, *args):\n",
    "    return self.target_fn(args[0], **self.target_params)\n",
    "\n",
    "\n",
    "\n",
    "class ImageExplanation(object):\n",
    "    def __init__(self, image, segments):\n",
    "        \"\"\"Init function.\n",
    "        Args:\n",
    "            image: 3d numpy array\n",
    "            segments: 2d numpy array, with the output from skimage.segmentation\n",
    "        \"\"\"\n",
    "        self.image = image\n",
    "        self.segments = segments\n",
    "        self.intercept = {}\n",
    "        self.local_exp = {}\n",
    "        self.local_pred = None\n",
    "\n",
    "    def get_image_and_mask(self, label, positive_only=True, negative_only=False, hide_rest=False,\n",
    "                           num_features=5, min_weight=0.):\n",
    "        \"\"\"Init function.\n",
    "        Args:\n",
    "            label: label to explain\n",
    "            positive_only: if True, only take superpixels that positively contribute to\n",
    "                the prediction of the label.\n",
    "            negative_only: if True, only take superpixels that negatively contribute to\n",
    "                the prediction of the label. If false, and so is positive_only, then both\n",
    "                negativey and positively contributions will be taken.\n",
    "                Both can't be True at the same time\n",
    "            hide_rest: if True, make the non-explanation part of the return\n",
    "                image gray\n",
    "            num_features: number of superpixels to include in explanation\n",
    "            min_weight: minimum weight of the superpixels to include in explanation\n",
    "        Returns:\n",
    "            (image, mask), where image is a 3d numpy array and mask is a 2d\n",
    "            numpy array that can be used with\n",
    "            skimage.segmentation.mark_boundaries\n",
    "        \"\"\"\n",
    "        if label not in self.local_exp:\n",
    "            raise KeyError('Label not in explanation')\n",
    "        if positive_only & negative_only:\n",
    "            raise ValueError(\"Positive_only and negative_only cannot be true at the same time.\")\n",
    "        segments = self.segments\n",
    "        image = self.image\n",
    "        exp = self.local_exp[label]\n",
    "        mask = np.zeros(segments.shape, segments.dtype)\n",
    "        if hide_rest:\n",
    "            temp = np.zeros(self.image.shape)\n",
    "        else:\n",
    "            temp = self.image.copy()\n",
    "        if positive_only:\n",
    "            fs = [x[0] for x in exp\n",
    "                  if x[1] > 0 and x[1] > min_weight][:num_features]\n",
    "        if negative_only:\n",
    "            fs = [x[0] for x in exp\n",
    "                  if x[1] < 0 and abs(x[1]) > min_weight][:num_features]\n",
    "        if positive_only or negative_only:\n",
    "            for f in fs:\n",
    "                temp[segments == f] = image[segments == f].copy()\n",
    "                mask[segments == f] = 1\n",
    "            return temp, mask\n",
    "        else:\n",
    "            for f, w in exp[:num_features]:\n",
    "                if np.abs(w) < min_weight:\n",
    "                    continue\n",
    "                c = 0 if w < 0 else 1\n",
    "                mask[segments == f] = -1 if w < 0 else 1\n",
    "                temp[segments == f] = image[segments == f].copy()\n",
    "                temp[segments == f, c] = np.max(image)\n",
    "            return temp, mask\n",
    "\n",
    "\n",
    "class LimeImageExplainer(object):\n",
    "    \"\"\"Explains predictions on Image (i.e. matrix) data.\n",
    "    For numerical features, perturb them by sampling from a Normal(0,1) and\n",
    "    doing the inverse operation of mean-centering and scaling, according to the\n",
    "    means and stds in the training data. For categorical features, perturb by\n",
    "    sampling according to the training distribution, and making a binary\n",
    "    feature that is 1 when the value is the same as the instance being\n",
    "    explained.\"\"\"\n",
    "\n",
    "    def __init__(self, kernel_width=.25, kernel=None, verbose=False,\n",
    "                 feature_selection='auto', random_state=None):\n",
    "        \"\"\"Init function.\n",
    "        Args:\n",
    "            kernel_width: kernel width for the exponential kernel.\n",
    "            If None, defaults to sqrt(number of columns) * 0.75.\n",
    "            kernel: similarity kernel that takes euclidean distances and kernel\n",
    "                width as input and outputs weights in (0,1). If None, defaults to\n",
    "                an exponential kernel.\n",
    "            verbose: if true, print local prediction values from linear model\n",
    "            feature_selection: feature selection method. can be\n",
    "                'forward_selection', 'lasso_path', 'none' or 'auto'.\n",
    "                See function 'explain_instance_with_data' in lime_base.py for\n",
    "                details on what each of the options does.\n",
    "            random_state: an integer or numpy.RandomState that will be used to\n",
    "                generate random numbers. If None, the random state will be\n",
    "                initialized using the internal numpy seed.\n",
    "        \"\"\"\n",
    "        kernel_width = float(kernel_width)\n",
    "\n",
    "        if kernel is None:\n",
    "            def kernel(d, kernel_width):\n",
    "                return np.sqrt(np.exp(-(d ** 2) / kernel_width ** 2))\n",
    "\n",
    "        kernel_fn = partial(kernel, kernel_width=kernel_width)\n",
    "\n",
    "        self.random_state = check_random_state(random_state)\n",
    "        self.feature_selection = feature_selection\n",
    "        self.base = lime_base.LimeBase(kernel_fn, verbose, random_state=self.random_state)\n",
    "\n",
    "    def explain_instance(self, image, classifier_fn, labels=(1,),\n",
    "                         hide_color=0,\n",
    "                         top_labels=5, num_features=10000, num_samples=1000,\n",
    "                         batch_size=10,\n",
    "                         segmentation_fn=None, k_size = 100,\n",
    "                         stack = 20,\n",
    "                         distance_metric='cosine',\n",
    "                         model_regressor=None,\n",
    "                         random_seed=None):\n",
    "        global  yuhao_exp \n",
    "        global  yuhao_score \n",
    "        global  yuhao_origpred \n",
    "        global  yuhao_label\n",
    "        global  yuhao_distance\n",
    "        global  yuhao_segment\n",
    "        global  yuhao_data\n",
    "        \"\"\"Generates explanations for a prediction.\n",
    "        First, we generate neighborhood data by randomly perturbing features\n",
    "        from the instance (see __data_inverse). We then learn locally weighted\n",
    "        linear models on this neighborhood data to explain each of the classes\n",
    "        in an interpretable way (see lime_base.py).\n",
    "        Args:\n",
    "            image: 3 dimension RGB image. If this is only two dimensional,\n",
    "                we will assume it's a grayscale image and call gray2rgb.\n",
    "            classifier_fn: classifier prediction probability function, which\n",
    "                takes a numpy array and outputs prediction probabilities.  For\n",
    "                ScikitClassifiers , this is classifier.predict_proba.\n",
    "            labels: iterable with labels to be explained.\n",
    "            hide_color: TODO\n",
    "            top_labels: if not None, ignore labels and produce explanations for\n",
    "                the K labels with highest prediction probabilities, where K is\n",
    "                this parameter.\n",
    "            num_features: maximum number of features present in explanation\n",
    "            num_samples: size of the neighborhood to learn the linear model\n",
    "            batch_size: TODO\n",
    "            distance_metric: the distance metric to use for weights.\n",
    "            model_regressor: sklearn regressor to use in explanation. Defaults\n",
    "            to Ridge regression in LimeBase. Must have model_regressor.coef_\n",
    "            and 'sample_weight' as a parameter to model_regressor.fit()\n",
    "            segmentation_fn: SegmentationAlgorithm, wrapped skimage\n",
    "            segmentation function\n",
    "            random_seed: integer used as random seed for the segmentation\n",
    "                algorithm. If None, a random integer, between 0 and 1000,\n",
    "                will be generated using the internal random number generator.\n",
    "        Returns:\n",
    "            An ImageExplanation object (see lime_image.py) with the corresponding\n",
    "            explanations.\n",
    "        \"\"\"\n",
    "        if len(image.shape) == 2:\n",
    "            image = gray2rgb(image)\n",
    "        if random_seed is None:\n",
    "            random_seed = self.random_state.randint(0, high=1000)\n",
    "\n",
    "        if segmentation_fn is None:\n",
    "             segmentation_fn = SegmentationAlgorithm('felzenszwalb', scale=50, sigma=0.8, min_size = 2, multichannel=True)\n",
    "#             segmentation_fn = SegmentationAlgorithm('quickshift', kernel_size=1,\n",
    "#                                                    max_dist=5, ratio=0.2,\n",
    "#                                                    random_seed=random_seed)\n",
    "            # segmentation_fn = SegmentationAlgorithm('slic',n_segments=k_size, compactness=1000, max_iter=5, sigma=0.8, channel_axis = -1)\n",
    "        # try:\n",
    "          #  segmentation_fn = SegmentationAlgorithm('slic',n_segments=k_size, compactness=1000, max_iter=5, sigma=0.8, channel_axis = -1)\n",
    "          #  segments = segmentation_fn(image)\n",
    "             # segments = np.arange(0,k_size).reshape((int(np.sqrt(k_size)),int(np.sqrt(k_size))))\n",
    "             segments = np.arange(0,k_size).reshape(C1, C2)\n",
    "             yuhao_segment = segments \n",
    "        # except ValueError as e:\n",
    "        #     raise e\n",
    "\n",
    "        fudged_image = image.copy()\n",
    "        print(\"fudged image shape\", fudged_image.shape)\n",
    "        if hide_color is None:\n",
    "            for x in np.unique(segments):\n",
    "                fudged_image[segments == x] = (\n",
    "                    np.mean(image[segments == x][:, 0]),\n",
    "                    np.mean(image[segments == x][:, 1]),\n",
    "                    np.mean(image[segments == x][:, 2]))\n",
    "        else:\n",
    "            fudged_image[:] = hide_color\n",
    "\n",
    "        top = labels\n",
    "\n",
    "        data, labels = self.data_labels(image, fudged_image, segments, stack,\n",
    "                                        classifier_fn, num_samples,\n",
    "                                        batch_size=batch_size)\n",
    "        yuhao_data = data\n",
    "\n",
    "        distances = sklearn.metrics.pairwise_distances(\n",
    "            data,\n",
    "            data[0].reshape(1, -1),\n",
    "            metric=distance_metric\n",
    "        ).ravel()\n",
    "        yuhao_distance = distances\n",
    "\n",
    "        ret_exp = ImageExplanation(image, segments)\n",
    "        if top_labels:\n",
    "            top = np.argsort(labels[0])[-top_labels:]\n",
    "            ret_exp.top_labels = list(top)\n",
    "            ret_exp.top_labels.reverse()\n",
    "        for label in top:\n",
    "            (ret_exp.intercept[label],\n",
    "             ret_exp.local_exp[label],\n",
    "             ret_exp.score, ret_exp.local_pred) = self.base.explain_instance_with_data(\n",
    "                data, labels, distances, label, num_features,\n",
    "                model_regressor=model_regressor,\n",
    "                feature_selection='auto')\n",
    "#            print(top)\n",
    "            yuhao_label = labels\n",
    "            yuhao_exp = ret_exp.local_exp\n",
    "            yuhao_score = ret_exp.score\n",
    "            yuhao_origpred = ret_exp.local_pred\n",
    "#            print(ret_exp.local_exp)\n",
    "#            print(ret_exp.score)\n",
    "#            print(ret_exp.local_pred)\n",
    "            #self.feature_selection\n",
    "        return ret_exp\n",
    "\n",
    "\n",
    "    def data_labels(self,\n",
    "                    image,\n",
    "                    fudged_image,\n",
    "                    segments,\n",
    "                    stack,\n",
    "                    classifier_fn,\n",
    "                    num_samples,\n",
    "                    batch_size=10):\n",
    "        \"\"\"Generates images and predictions in the neighborhood of this image.\n",
    "        Args:\n",
    "            image: 3d numpy array, the image\n",
    "            fudged_image: 3d numpy array, image to replace original image when\n",
    "                superpixel is turned off\n",
    "            segments: segmentation of the image\n",
    "            classifier_fn: function that takes a list of images and returns a\n",
    "                matrix of prediction probabilities\n",
    "            num_samples: size of the neighborhood to learn the linear model\n",
    "            batch_size: classifier_fn will be called on batches of this size.\n",
    "        Returns:\n",
    "            A tuple (data, labels), where:\n",
    "                data: dense num_samples * num_superpixels\n",
    "                labels: prediction probabilities matrix\n",
    "        \"\"\"\n",
    "        global yuhao_imgs\n",
    "        \n",
    "        yuhao_imgs = []\n",
    "        n_features = np.unique(segments).shape[0] * stack\n",
    "        data = self.random_state.randint(0, 2, num_samples * n_features)\\\n",
    "            .reshape((num_samples, n_features))\n",
    "        labels = []\n",
    "        data[0, :] = 1\n",
    "        imgs = []\n",
    "      \n",
    "        for row in tqdm(data):\n",
    "            temp = copy.deepcopy(image)\n",
    "            zeros = np.where(row == 0)[0]\n",
    "            mask = np.zeros((stack, segments.shape[0],segments.shape[1])).astype(bool)\n",
    "            print(\"mask shape\", mask.shape)\n",
    "            for z in zeros:\n",
    "                mask[int(z/1935), segments == z%1935] = True\n",
    "            temp[mask] = fudged_image[mask]\n",
    "            imgs.append(temp)\n",
    "            if len(imgs) == batch_size:\n",
    "                preds = classifier_fn(np.array(imgs))\n",
    "                labels.extend(preds)\n",
    "                yuhao_imgs.append(np.array(imgs))\n",
    "                imgs = []\n",
    "            \n",
    "        if len(imgs) > 0:\n",
    "            preds = classifier_fn(np.array(imgs))\n",
    "            labels.extend(preds)\n",
    "            yuhao_imgs.append(np.array(imgs))\n",
    "        return data, np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059560f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMPY_INPUT_DATA = TENSOR_INPUT_DATA_PRINTING.numpy()\n",
    "\n",
    "# seg_fn = 'slic'\n",
    "# LX = np.load('cwm_markers_dataset/LX.npy')\n",
    "# LY = np.load('cwm_markers_dataset/LXY.npy')\n",
    "\n",
    "LX = NUMPY_INPUT_DATA[:,:,:,:]\n",
    "LY = y_target[:]\n",
    "print(\"LX shape\", LX.shape)\n",
    "print(\"LY shape\", LY.shape)\n",
    "model = load_model('porosity_bin_model_printing_fold_no_4.h5')\n",
    "pred = model.predict(LX)\n",
    "\n",
    "# LY = np.array([1 if a == 0 else 0 for a in Y])\n",
    "pred_class = [0 if a < 0.5 else 1 for a in pred]\n",
    "correct_X_ind = list(np.where(pred_class==LY)[0])\n",
    "print(len(correct_X_ind))\n",
    "\n",
    "import lime\n",
    "from lime import lime_image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "sam_l = 1\n",
    "stack = 4  # Tensor size\n",
    "C1 = 129  # NUmber of frequency bands in spectrogram\n",
    "C2 = 15   # Number of time intervals in spectrogram\n",
    "stack_shift = 1\n",
    "window_length = 300\n",
    "train_size = 0.9\n",
    "n_class = 2\n",
    "k_size = C1*C2\n",
    "num_s = 3500   # Number of perturbations\n",
    "\n",
    "channel = 4    # Number of data channels \n",
    "inter_period = 5400\n",
    "lag = 0\n",
    "fs = 2000\n",
    "w_shift = int(window_length/2)\n",
    "batch_size = 32\n",
    "epochs = 2000   # Number of epochs\n",
    "lr = 0.0002     # Learning rate\n",
    "\n",
    "# Rename file directories \n",
    "fname_out = 'C:\\\\Users\\.............\\...'\n",
    "data_f = fname_out +'/'+str(sam_l)+'s_continuous/inter_'+str(inter_period)+'s/with_seizure/DPGMM_C'+str(C)+'/window'+str(window_length)+'/shift'+str(w_shift)\n",
    "save_data_f = data_f +'/stack'+str(stack)+'_shift'+str(stack_shift)+'/scaled01'        #############\n",
    "save_f = fname_out +'/'+str(n_class)+'class/CNN_best_train_lr'+str(lr)+'_epoch'+str(epochs) \n",
    "\n",
    "\n",
    "exp = LimeImageExplainer()\n",
    "\n",
    "score = []\n",
    "f_name1 = save_f+'/detailed_lime/numseg'+str(k_size)+'_nums'+str(num_s)+'/local results'\n",
    "if not os.path.exists(f_name1):\n",
    "    os.makedirs(f_name1)\n",
    "\n",
    "for sample_ind in range(len(correct_X_ind)):\n",
    "# for sample_ind in range(0,1):\n",
    "\n",
    "    # Explain a single sample\n",
    "    sample = LX[sample_ind, :, :, :].astype('double')\n",
    "    y_samp = LY[sample_ind]       \n",
    "    explanation = exp.explain_instance(sample, model, top_labels=n_class, hide_color = 0, num_samples=num_s, k_size = k_size, stack = stack) \n",
    "    score.append(explanation.score)\n",
    "  \n",
    "    # only study the good quality explanations\n",
    "    if explanation.score >= 0.75 and explanation.score <= 1:\n",
    "\n",
    "        # save weights and lime ratios\n",
    "        weightRatio = []\n",
    "        for i, e in explanation.local_exp[explanation.top_labels[0]]:\n",
    "            weightRatio.append((i, e, explanation.local_pred, explanation.intercept, explanation.top_labels[0]))\n",
    "  \n",
    "        WR = pd.DataFrame(weightRatio)\n",
    "        WR.to_csv(f_name1+'/'+str(sample_ind)+'_weightRatioPred_score'+str(round(explanation.score,2))+'_numseg'+str(len(explanation.local_exp[explanation.top_labels[0]]))+'.csv', index = False)\n",
    "        \n",
    "score = np.array(score)\n",
    "np.save(save_f+'/detailed_lime/numseg'+str(k_size)+'_nums'+str(num_s)+'/score.npy',score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
